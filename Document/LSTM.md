# 长短期记忆网络

长短期记忆网络是 RNN 的进化网络，RNN 与CNN 的不同之处在于，它们包含反馈循环，编码一个时间序列的上下文信息，
给定一个输入序列 $x=(x_1,\cdots ,x_T)$ ， $x_t$ 是 CNN 在 t 时刻提取的特征向量，可以得到循环层的隐藏状态 $h = (h_1,\cdots ,h_T )$ 和输出 $y=(y_1,\cdots ,y_T)$ ：

$$\begin{align*}
h_t &= H(W_{ih}x_t + W_{hh}h_{t-1} + b_n) \\
y_t &= W_{ho}h_t + b_o
\end{align*}$$

其中 $W_s$ 是连接输入层、隐藏层和输出层的权值矩阵， $b$ 是偏差向量， $H$ 是隐藏层的激活函数。隐藏状态 $h_t$ 按时间步长传递，输出$y_t$ 被输入到 softmax 层，提供每一帧的手势概率。但当在长序列上学习时，标准的RNN 可能会遭受数值不稳定，即梯度消失或爆炸的问题。为了避免了长期依赖问题，且可以有效地提取序列中的长距离依赖信息。相比传统的 RNN，该网络引进了一种新的循环单元

传统循环单元是通过应用非线性函数和计算输入序列的加权，LSTM 与其不同的是，利用巧妙设计的门结构，实现对输入的信息进行保护和控制。LSTM 使用内存单元通过特殊的门来存储、修改和访问内部状态，从而允许长序列时间建模，具体门连接和内存单元位置之间的关系如图所示。

<div align = 'center'><img src = '../Document\pic\屏幕截图 2023-11-25 222351.png' width = '450' title = 'LTSM 结构'></div>