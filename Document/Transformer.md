# Transformer

Transformer 模型是一个自然语言处理模型，其网络架构模如图所示。传统的循环神经网络在处理长时间序列的问题时，会出现梯度消失与梯度爆炸等问题，LSTM 网络对长序列的记忆能力有了较大改善，但对于超长序列仍存在问题。注意力机制的提出消除了输入输出之间距离的影响，根据长时间序列的依赖关系进行序列建模。Transformer 模型利用全局自注意力机制，能够捕捉到输入序列和输出序列的依赖关系。

<div align = 'center'><img src = '../Document\pic\屏幕截图 2023-11-25 223645.png' width = '450' title = 'transformer 结构 1'></div>

Transformer 模型本质是 N 层编码器与解码器的结构，编码器包含多头注意力机制和前馈神经网络两个子层，解码器还额外包含遮掩多头注意力机制子层。编码器负责将输入的手势特征序列进行编码，主要依靠多头自注意力机制，模型结构见下图。通过计算特征向量间的相似度来表征序列相关性，以解决长时间序列的依赖。自注意力机制通过缩放点积注意力来计算特征向量的注意力值。

<div align = 'center'><img src = '../Document\pic\屏幕截图 2023-11-25 230344.png' width = '400' title = 'transformer 结构 2'></div>